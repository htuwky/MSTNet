import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import sys

# ç¡®ä¿èƒ½å¯¼å…¥ config
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config


class MSTNetDataset(Dataset):
    def __init__(self, mode='train', split_ratio=0.8):
        """
        MSTNet æ•°æ®é›†åŠ è½½å™¨
        Args:
            mode: 'train' æˆ– 'val'
            split_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        """
        self.temp_dir = config.TEMP_FEATURE_DIR
        self.max_len = config.MAX_SEQ_LEN  # 512

        # 1. æ‰«ææ‰€æœ‰ä¸»ç‰¹å¾æ–‡ä»¶ (ä¾‹å¦‚ 001.npy, 002.npy)
        # æ’é™¤æ‰å¸¦ _motion çš„ï¼Œæˆ‘ä»¬é€šè¿‡ä¸»æ–‡ä»¶å…³è”è¿åŠ¨æ–‡ä»¶
        all_files = sorted([
            f for f in os.listdir(self.temp_dir)
            if f.endswith('.npy') and '_motion' not in f
        ])

        if not all_files:
            raise FileNotFoundError(f"âŒ åœ¨ {self.temp_dir} æ²¡æ‰¾åˆ°ç‰¹å¾æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæå–è„šæœ¬ï¼")

        # 2. åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
        split_idx = int(len(all_files) * split_ratio)
        if mode == 'train':
            self.file_list = all_files[:split_idx]
        else:
            self.file_list = all_files[split_idx:]

        print(f"ğŸ“¦ MSTNet Dataset [{mode}]: åŠ è½½äº† {len(self.file_list)} ä¸ªæ ·æœ¬")

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        # --- 1. å®šä½æ–‡ä»¶è·¯å¾„ ---
        visual_file = self.file_list[idx]
        subject_id = visual_file.replace('.npy', '')
        motion_file = f"{subject_id}_motion.npy"

        visual_path = os.path.join(self.temp_dir, visual_file)
        motion_path = os.path.join(self.temp_dir, motion_file)

        # --- 2. åŠ è½½æ•°æ®å­—å…¸ ---
        # v_data åŒ…å«: 'local', 'global', 'timestamp'
        v_data = np.load(visual_path, allow_pickle=True).item()
        # m_data åŒ…å«: 'motion', 'physio'
        m_data = np.load(motion_path, allow_pickle=True).item()

        # --- 3. æå–å„ä¸ªæµæ‰€éœ€çš„åŸå§‹æ•°æ® ---
        local_feat = v_data['local']  # [Seq, 512] -> ç”¨äº Temporal å’Œ GNN
        global_feat = v_data['global']  # [Seq, 512] -> ç”¨äº Temporal
        motion_feat = m_data['motion']  # [Seq, 6]   -> ç”¨äº Motion æµ
        physio_feat = m_data['physio']  # [Seq, 3] (x, y, t) -> ç”¨äº Temporal ä½ç½®å’Œ GNN æ„å›¾
        # å°† physio_feat çš„ç¬¬ 3 åˆ— (Index 2) ä»åŸå§‹ç§’æ•°è½¬æ¢ä¸º 0-1 æ¯”ä¾‹
        # è¿™æ · (x, y, t) å…¨éƒ¨é”æ­»åœ¨ [0, 1] ä¹‹é—´
        physio_feat[:, 2] /= config.VIDEO_DURATION
        # æ ‡ç­¾é€»è¾‘ï¼šå‡è®¾å‰ä¸€åŠåºå·ä¸ºå¥åº·(0)ï¼Œåä¸€åŠä¸ºæ‚£ç—…(1)
        label = 1 if int(subject_id) > (config.NUM_SIMULATED_PEOPLE // 2) else 0

        # --- 4. ç»Ÿä¸€æ—¶åºå¤„ç† (æˆªæ–­æˆ–å¡«å……) ---
        # æŒ‰ç…§ä½ è¦æ±‚çš„ï¼šå³ä½¿æœ‰å¼‚å¸¸å€¼ç‚¹ï¼Œæˆ‘ä»¬ä¹Ÿä¿ç•™æ—¶é—´æˆ³å ä½ï¼Œç»Ÿä¸€åˆ° 512
        curr_len = local_feat.shape[0]

        if curr_len >= self.max_len:
            # æˆªæ–­åˆ° 512
            local_feat = local_feat[:self.max_len]
            global_feat = global_feat[:self.max_len]
            motion_feat = motion_feat[:self.max_len]
            physio_feat = physio_feat[:self.max_len]
            # Padding Mask: å…¨éƒ¨ä¸º False (è¡¨ç¤ºå…¨æ˜¯çœŸå®æ•°æ®)
            mask = torch.zeros(self.max_len, dtype=torch.bool)
        else:
            # å¡«å…… (Zero Padding) åˆ° 512
            pad_len = self.max_len - curr_len

            local_feat = np.pad(local_feat, ((0, pad_len), (0, 0)))
            global_feat = np.pad(global_feat, ((0, pad_len), (0, 0)))
            motion_feat = np.pad(motion_feat, ((0, pad_len), (0, 0)))
            physio_feat = np.pad(physio_feat, ((0, pad_len), (0, 0)))

            # Mask: å‰é¢æœ‰æ•ˆä½æ˜¯ Falseï¼Œåé¢å¡«å……ä½æ˜¯ True
            mask = torch.zeros(self.max_len, dtype=torch.bool)
            mask[curr_len:] = True

        # --- 5. æœ€ç»ˆæ‰“åŒ…è¿”å› ---
        # è¿”å›ä¸€ä¸ªå¤§å­—å…¸ï¼Œæ–¹ä¾¿ä¸»æ¨¡å‹åˆ†å‘ç»™ä¸‰ä¸ªæµ
        return {
            "temporal_input": {
                "local": torch.from_numpy(local_feat).float(),
                "global": torch.from_numpy(global_feat).float(),
                "physio": torch.from_numpy(physio_feat).float()
            },
            "motion_input": torch.from_numpy(motion_feat).float(),
            "gnn_input": {
                "local": torch.from_numpy(local_feat).float(),
                "coords": torch.from_numpy(physio_feat).float()
            },
            "mask": mask,  # å½¢çŠ¶ [512]
            "label": torch.tensor(label, dtype=torch.long)
        }


def get_mstnet_loaders(batch_size=None):
    """
    ä¾¿æ·è·å–è®­ç»ƒå’ŒéªŒè¯ DataLoader çš„æ¥å£
    """
    if batch_size is None:
        batch_size = config.BATCH_SIZE  # ä½¿ç”¨ config.py é‡Œçš„ 64

    train_ds = MSTNetDataset(mode='train')
    val_ds = MSTNetDataset(mode='val')

    train_loader = DataLoader(
        train_ds,
        batch_size=batch_size,
        shuffle=True,
        num_workers=config.NUM_WORKERS  #
    )
    val_loader = DataLoader(
        val_ds,
        batch_size=batch_size,
        shuffle=False,
        num_workers=config.NUM_WORKERS  #
    )

    return train_loader, val_loader


# ================= å•å…ƒæµ‹è¯• (Unit Test) =================
if __name__ == "__main__":
    try:
        train_l, val_l = get_mstnet_loaders(batch_size=4)
        sample_batch = next(iter(train_l))

        print("\nâœ… DataLoader æµ‹è¯•æˆåŠŸ!")
        print(f"Batch Label Shape: {sample_batch['label'].shape}")
        print(f"Temporal Local Shape: {sample_batch['temporal_input']['local'].shape}")
        print(f"Motion Input Shape: {sample_batch['motion_input'].shape}")
        print(f"GNN Coords Shape: {sample_batch['gnn_input']['coords'].shape}")
        print(f"Mask Sample: {sample_batch['mask'][0][:10]}... (Falseä»£è¡¨æœ‰æ•ˆ)")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")